[DATA]
file = data/test_smiles.txt
# which fingerprints to use
fingerprints = ['Morgan_2']
# properties used for co-training from the latent space
properties = ['all']
train_val_split = 0.8
# here rather than in Train as it is used to construct generators
batch_size = 128

[MODEL_ENCODER]
# last layer have always number of units corresponding to latent space and linear activation
# separate dense layers for encoder
# units_ind = [[500, 300]]
# common units after concat from in channels: for single input define just this
units_common = [500, 300]
dropout = 0.2
activation = relu
# size of the latent space
latent_space = 100

[MODEL_DECODER]
# last layer have always number of units corresponding to input vectors and sigmoid activation
# common units from latent space: for single input define just this
units_common = [200, 200]
# separate dense layers for decoding channel
#units_ind = [300, 500]
dropout = 0.2
activation = relu
latent_activation = sigmoid

[MODEL_PROPERTIES]
# last layer have always number of units corresponding to number of properties to predict and linear activation
# batch norm layer at the beginning and end?
batch_norm = [True, False]
units_common = [100, 100, 50, 50]
dropout = 0.2
activation = relu

[TRAIN]
epochs = 20
optimizer = adam
learning_rate = 0.001
# parameters for reducing of learning rate on plateau
lr_patience = 5
lr_factor = 0.5
lr_min = 0.00001
# loss for the training of autencoder
loss_fp = ['binary_crossentropy']
# loss for co-trained property prediction
loss_pp = ['mse']
# weights for fp and prop losses: one entry per every fp and one common for properties
loss_weights = [1,0.01]
metrics_fp = ['binary_accuracy', 'reconstructed_cosine_similarity', 'reconstructed_tanimoto_similarity']
metrics_pp = ['mape']
# period when model is saved
save_model_period = 10
# if early stop when no imporvemnts in val loss for 15 epochs
early_stop = 0